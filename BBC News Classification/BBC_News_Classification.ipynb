{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## BBC News Classification\n",
    "\n",
    "Completed as part of CU Boulder's Unsupervised Algorithms in Machine Learning course."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Project dependencies\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import math\n",
    "import time\n",
    "import re\n",
    "import spacy\n",
    "from collections import defaultdict\n",
    "# Vectorizing word data\n",
    "# importing all necessary modules\n",
    "import nltk\n",
    "from nltk.tokenize import sent_tokenize, word_tokenize\n",
    "import warnings\n",
    "warnings.filterwarnings(action = 'ignore')\n",
    "\n",
    "import gensim\n",
    "from gensim.models import Word2Vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data import\n",
    "train = pd.read_csv(\"data/BBC News Train.csv\")\n",
    "test = pd.read_csv(\"data/BBC News Test.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exploratory Data Analysis (EDA)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   ArticleId                                               Text  Category\n",
      "0       1833  worldcom ex-boss launches defence lawyers defe...  business\n",
      "1        154  german business confidence slides german busin...  business\n",
      "2       1101  bbc poll indicates economic gloom citizens in ...  business\n",
      "3       1976  lifestyle  governs mobile choice  faster  bett...      tech\n",
      "4        917  enron bosses in $168m payout eighteen former e...  business\n",
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 1490 entries, 0 to 1489\n",
      "Data columns (total 3 columns):\n",
      " #   Column     Non-Null Count  Dtype \n",
      "---  ------     --------------  ----- \n",
      " 0   ArticleId  1490 non-null   int64 \n",
      " 1   Text       1490 non-null   object\n",
      " 2   Category   1490 non-null   object\n",
      "dtypes: int64(1), object(2)\n",
      "memory usage: 35.0+ KB\n",
      "\n",
      "Train: None\n",
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 735 entries, 0 to 734\n",
      "Data columns (total 2 columns):\n",
      " #   Column     Non-Null Count  Dtype \n",
      "---  ------     --------------  ----- \n",
      " 0   ArticleId  735 non-null    int64 \n",
      " 1   Text       735 non-null    object\n",
      "dtypes: int64(1), object(1)\n",
      "memory usage: 11.6+ KB\n",
      "\n",
      "Test: None\n"
     ]
    }
   ],
   "source": [
    "\n",
    "print(train.head(5))\n",
    "print(\"\\nTrain:\", train.info())\n",
    "print(\"\\nTest:\", test.info())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ArticleId 0 0\n",
      "Text 0 0\n",
      "Category 0 0\n",
      "ArticleId 0 0\n",
      "Text 0 0\n"
     ]
    }
   ],
   "source": [
    "# Check for any null/null-like values\n",
    "null_like = [np.nan, None, [], {}, 'NaN', 'Null','NULL','None','NA','?','-', '.', '', ' ', '   ']\n",
    "\n",
    "for df in [train, test]:\n",
    "    for c in df.columns:\n",
    "        string_null = np.array([x in null_like for x in df[c]])\n",
    "        print(c, df[c].isnull().sum(), string_null.sum())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cleaned in: 36.52 secs\n"
     ]
    }
   ],
   "source": [
    "# Top 20 most frequent words:\n",
    "# ['the', '.', 'to', 'of', 'and', 'a', 'in', 's', 'for', 'is', 'that', 'it', 'on', 'said', 'was', 'he', 'be', 'with', 'has', 'as']\n",
    "# So we need to do some cleaning!\n",
    "\n",
    "\n",
    "# Using helpers from: https://www.kaggle.com/code/pierremegret/gensim-word2vec-tutorial/notebook\n",
    "nlp = spacy.load('en_core_web_sm', disable=['ner', 'parser']) # disabling Named Entity Recognition for speed\n",
    "\n",
    "def cleaning(doc):\n",
    "    # Lemmatizes and removes stopwords\n",
    "    # doc needs to be a spacy Doc object\n",
    "    txt = [token.lemma_ for token in doc if not token.is_stop]\n",
    "    return ' '.join(txt)\n",
    "\n",
    "brief_cleaning = (re.sub(\"[^A-Za-z']+\", ' ', str(row)).lower() for row in train['Text'])\n",
    "\n",
    "t = time.time()\n",
    "\n",
    "train['Clean Text'] = [cleaning(doc) for doc in nlp.pipe(brief_cleaning, batch_size=5000)]\n",
    "\n",
    "print('Cleaned in: {} secs'.format(round((time.time() - t), 2)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "showing info https://raw.githubusercontent.com/nltk/nltk_data/gh-pages/index.xml\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "# for installing missing corpora if needed\n",
    "nltk.download()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "18437\n",
      "['worldcom', 'ex', 'boss', 'launch', 'defence', 'lawyer', 'defend', 'worldcom', 'chief', 'bernie', 'ebber', 'battery', 'fraud', 'charge', 'call', 'company', 'whistleblow', 'witness', 'cynthia', 'cooper', 'worldcom', 's', 'ex', 'head', 'internal', 'accounting', 'alert', 'director', 'irregular', 'accounting', 'practice', 'telecom', 'giant', 'warning', 'lead', 'collapse', 'firm', 'follow', 'discovery', 'bn', 'bn', 'accounting', 'fraud', 'mr', 'ebber', 'plead', 'guilty', 'charge', 'fraud', 'conspiracy', 'prosecution', 'lawyer', 'argue', 'mr', 'ebber', 'orchestrate', 'series', 'accounting', 'trick', 'worldcom', 'order', 'employee', 'hide', 'expense', 'inflate', 'revenue', 'meet', 'wall', 'street', 'earning', 'estimate', 'ms', 'cooper', 'run', 'consulting', 'business', 'tell', 'jury', 'new', 'york', 'wednesday', 'external', 'auditor', 'arthur', 'andersen', 'approve', 'worldcom', 's', 'accounting', 'early', 'say', 'andersen', 'give', 'green', 'light', 'procedure', 'practice', 'worldcom', 'mr', 'ebber', 's', 'lawyer', 'say', 'unaware', 'fraud', 'argue', 'auditor', 'alert', 'problem', 'ms', 'cooper', 'say', 'shareholder', 'meeting', 'mr', 'ebber', 'pass', 'technical', 'question', 'company', 's', 'finance', 'chief', 'give', 'brief', 'answer', 'prosecution', 's', 'star', 'witness', 'worldcom', 'financial', 'chief', 'scott', 'sullivan', 'say', 'mr', 'ebber', 'order', 'accounting', 'adjustment', 'firm', 'tell', 'hit', 'book', 'ms', 'cooper', 'say', 'mr', 'sullivan', 'mention', 'uncomfortable', 'worldcom', 's', 'accounting', 'audit', 'committee', 'meeting', 'mr', 'ebber', 'face', 'jail', 'sentence', 'year', 'convict', 'charge', 'face', 'worldcom', 'emerge', 'bankruptcy', 'protection', 'know', 'mci', 'week', 'mci', 'agree', 'buyout', 'verizon', 'communication', 'deal', 'value', 'bn'] ['hewitt', 'overcome', 'wobble', 'sydney', 'lleyton', 'hewitt', 'give', 'perfect', 'preparation', 'week', 's', 'australian', 'open', 'victory', 'ivo', 'minar', 'final', 'sydney', 'international', 'defend', 'champion', 'brush', 'aside', 'czech', 'qualifi', 'australian', 'world', 'number', 'stroll', 'game', 'match', 'shock', 'minar', 'win', 'seed', 'rattle', 'recover', 'close', 'set', 'race', 'victory', 'exactly', 'hour', 'strange', 'match', 'momentum', 'swing', 'say', 'hewitt', 'feel', 'like', 'come', 'block', 'extremely', 'loosen', 'bit', 'get', 'try', 'settle', 'notch', 'able', 'hewitt', 'lift', 'sydney', 'title', 'time', 'year', 'keep', 'get', 'well', 'well', 'year', 'hewitt', 'say', 've', 'play', 'tournament', 'time', 've', 'win', 'time', 'hewitt', 'go', 'final', 'short', 'price', 'favourite', 'clinch', 'th', 'career', 'title', 'drop', 'set', 'week', 'set', 'meet', 'frenchman', 'arnaud', 'clement', 'round', 'australian', 'open', 'minar', 'rank', 'th', 'world', 'force', 'pull', 'australian', 'open', 'qualifying', 'draw', 'play', 'atp', 'final', 'big', 'success', 'say', 'year', 'old', 'nervous', 'night', 'couldn', 't', 'sleep', 've', 'play', 'player', 'want', 'play', 'qualify', 'melbourne', 's', 'saturday', 'm', 'main', 'draw', 'need', 'point', 'money', 'alicia', 'molik', 'take', 'samantha', 'stosur', 'australian', 'woman', 's', 'final', 'later', 'saturday']\n"
     ]
    }
   ],
   "source": [
    "# iterate through each sentence in the file\n",
    "data = []\n",
    "word_freq = defaultdict(int)\n",
    "\n",
    "for idx, article in train.iterrows():\n",
    "    text = article['Clean Text']\n",
    "    for i in sent_tokenize(text):\n",
    "        temp = []\n",
    "        # tokenize the sentence into words\n",
    "        for j in word_tokenize(i):\n",
    "            temp.append(j.lower())\n",
    "            word_freq[j.lower()] += 1\n",
    "        data.append(temp)\n",
    "\n",
    "# With stop words, unique word count was 28,178\n",
    "print(len(word_freq))\n",
    "print(data[0],data[20])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Top 20 most frequent:\n",
      " ['s', 'say', 'year', 'mr', 'new', 'people', 'm', 'good', 'win', 'time', 'game', 'film', 'world', 't', 'uk', 'come', 'government', 'play', 'go', 'work']\n"
     ]
    }
   ],
   "source": [
    "word_freq_sorted = sorted(word_freq, key=word_freq.get, reverse=True)\n",
    "print(\"Top 20 most frequent:\\n\", word_freq_sorted[:20])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Score 'lawyer' <-> 'defence':  0.98409194\n",
      "Score 'lawyer' <-> 'economy': 0.868916\n"
     ]
    }
   ],
   "source": [
    "\n",
    "model1 = gensim.models.Word2Vec(data, min_count = 5, vector_size = 200, window = 3)\n",
    "\n",
    "# Print results\n",
    "print(\"Score 'lawyer' <-> 'defence': \", model1.wv.similarity('lawyer', 'defence'))\n",
    "\n",
    "print(\"Score 'lawyer' <-> 'economy':\", model1.wv.similarity('lawyer', 'economy'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('economic', 0.993360698223114),\n",
       " ('spending', 0.9909805059432983),\n",
       " ('figure', 0.9868961572647095),\n",
       " ('taxis', 0.9825689196586609),\n",
       " ('cut', 0.9815003275871277),\n",
       " ('income', 0.9801750183105469),\n",
       " ('raise', 0.9791306257247925),\n",
       " ('bank', 0.9778202176094055),\n",
       " ('deficit', 0.977458119392395),\n",
       " ('china', 0.9764323830604553)]"
      ]
     },
     "execution_count": 90,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model1.wv.most_similar(positive=[\"economy\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [],
   "source": [
    "lists = sorted(word_freq.items()) # sorted by key, return a list of tuples\n",
    "x, y = zip(*lists) # unpack a list of pairs into two tuples\n",
    "\n",
    "plt.plot(x, y, color=\"purple\")\n",
    "plt.title(\"Word token frequency\")\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.0 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.0"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "aee8b7b246df8f9039afb4144a1f6fd8d2ca17a180786b69acc140d282b71a49"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
